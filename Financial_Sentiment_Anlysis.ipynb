{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c5aac7-959e-433e-ae69-8d51be3e26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import inflect\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121fb5bb-79c4-45d9-892d-235e4f97ddca",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6ed360-2fdf-440e-9887-1d25ba11a915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5837</th>\n",
       "      <td>RISING costs have forced packaging producer Hu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5838</th>\n",
       "      <td>Nordic Walking was first used as a summer trai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5839</th>\n",
       "      <td>According shipping company Viking Line , the E...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5840</th>\n",
       "      <td>In the building and home improvement trade , s...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>HELSINKI AFX - KCI Konecranes said it has won ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5842 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence Sentiment\n",
       "0     The GeoSolutions technology will leverage Bene...  positive\n",
       "1     $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2     For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3     According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4     The Swedish buyout firm has sold its remaining...   neutral\n",
       "...                                                 ...       ...\n",
       "5837  RISING costs have forced packaging producer Hu...  negative\n",
       "5838  Nordic Walking was first used as a summer trai...   neutral\n",
       "5839  According shipping company Viking Line , the E...   neutral\n",
       "5840  In the building and home improvement trade , s...   neutral\n",
       "5841  HELSINKI AFX - KCI Konecranes said it has won ...  positive\n",
       "\n",
       "[5842 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/pin.lyu/Documents/BC_Folder/NLP/Data/financial_data.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3beb6b-eddf-4d4b-833d-b9a5f3108a27",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbcb9227-7777-45a8-8e8f-68265b48de47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Types:\n",
      " ['positive' 'negative' 'neutral']\n",
      "\n",
      "Sentiment Counts:\n",
      " Sentiment\n",
      "neutral     3130\n",
      "positive    1852\n",
      "negative     860\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the types of dependent variable \n",
    "\n",
    "sentiment_types = df[\"Sentiment\"].unique()\n",
    "\n",
    "# Count occurrences of each sentiment type\n",
    "\n",
    "sentiment_counts = df[\"Sentiment\"].value_counts()\n",
    "\n",
    "# Results\n",
    "print(\"Sentiment Types:\\n\", sentiment_types)\n",
    "\n",
    "print(\"\\nSentiment Counts:\\n\", sentiment_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a67ec0-fa43-4cee-9c7c-6e8527d2b57c",
   "metadata": {},
   "source": [
    "_Comments_: unbalanced data, treatment needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8396e2-ac9e-49fa-bd79-f857739f4fff",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2498c11-bf59-46b6-8f46-8a7368410154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize number-to-word converter\n",
    "\n",
    "p = inflect.engine()\n",
    "\n",
    "# Function to replace numbers with words\n",
    "\n",
    "def replace_numbers(text):\n",
    "    \n",
    "    return re.sub(r'\\d+', lambda x: p.number_to_words(x.group()), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06b998c0-52fb-436b-b3ef-c7e291f91bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    text = text.lower()                                 # Lower case all words\n",
    "\n",
    "    text = replace_numbers(text)                        # Convert numbers to words\n",
    "    \n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)  # Remove punctuation\n",
    "    \n",
    "    tokens = word_tokenize(text)                        # Tokenize words\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))        # Activate stop words identifier\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()                    # Initialize lemmatizer\n",
    "    \n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]  # Lemmatize filtered words\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ff6496-9c1e-4781-8d30-28dc84923668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence  \\\n",
      "0  The GeoSolutions technology will leverage Bene...   \n",
      "1  $ESI on lows, down $1.50 to $2.50 BK a real po...   \n",
      "2  For the last quarter of 2010 , Componenta 's n...   \n",
      "3  According to the Finnish-Russian Chamber of Co...   \n",
      "4  The Swedish buyout firm has sold its remaining...   \n",
      "\n",
      "                                  Processed_Sentence  \n",
      "0  [geosolutions, technology, leverage, benefon, ...  \n",
      "1  [esi, low, onefifty, twofifty, bk, real, possi...  \n",
      "2  [last, quarter, two, thousand, ten, componenta...  \n",
      "3  [according, finnishrussian, chamber, commerce,...  \n",
      "4  [swedish, buyout, firm, sold, remaining, twent...  \n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the \"Sentence\" column\n",
    "\n",
    "df[\"Processed_Sentence\"] = df[\"Sentence\"].apply(preprocess)\n",
    "\n",
    "# Result\n",
    "\n",
    "print(df[[\"Sentence\", \"Processed_Sentence\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24d73803-fd9c-4485-9ad6-9ce1c5a1f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sentiment labels\n",
    "\n",
    "sentiment_num = {\"positive\": 1, \n",
    "                     \n",
    "                     \"neutral\": 0, \n",
    "                     \n",
    "                     \"negative\": 2}\n",
    "\n",
    "# Map numeric labels onto actual data\n",
    "\n",
    "df[\"Sentiment_NumLabel\"] = df[\"Sentiment\"].map(sentiment_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87246e2-17e4-45db-bea1-5f27a369878a",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "978fee6d-dfe5-455c-bebf-5447467e896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn list into string\n",
    "\n",
    "df[\"Processed_Sentence\"] = df[\"Processed_Sentence\"].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Split data with 80/20 rule\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Processed_Sentence\"], df[\"Sentiment_NumLabel\"], test_size=0.2, random_state=226)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93462185-7e87-4419-acd1-c9906aea078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4673,)\n",
      "X_test shape: (1169,)\n",
      "y_train shape: (4673,)\n",
      "y_test shape: (1169,)\n"
     ]
    }
   ],
   "source": [
    "# Dimension Check\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "X_test.shape\n",
    "\n",
    "y_train.shape\n",
    "\n",
    "y_test.shape\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "     \n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "     \n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "      \n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c46f9021-99c2-43d3-94ae-d9ffcc98fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [str(sentence) for sentence in X_train]  \n",
    "\n",
    "X_test = [str(sentence) for sentence in X_test]\n",
    "\n",
    "# Convert text into numerical features using TF-IDF\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=3000) \n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "188f238d-7d2d-4beb-bb4f-0adc02396517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate models\n",
    "\n",
    "models = {\n",
    "    \n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),  \n",
    "    \n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \n",
    "    \"XGBoost\": xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    \n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=42),\n",
    "\n",
    "    \"SVM\": SVC(kernel='linear', probability=True, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a671089-9ee6-4a3c-a673-7c3d53e764f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes...\n",
      "Results for Naive Bayes:\n",
      "Accuracy: 0.6972\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.94      0.80       653\n",
      "           1       0.72      0.54      0.61       351\n",
      "           2       0.52      0.07      0.12       165\n",
      "\n",
      "    accuracy                           0.70      1169\n",
      "   macro avg       0.65      0.52      0.51      1169\n",
      "weighted avg       0.68      0.70      0.65      1169\n",
      "\n",
      "Confusion Matrix:\n",
      "[[615  32   6]\n",
      " [158 189   4]\n",
      " [111  43  11]]\n",
      "--------------------------------------------------\n",
      "Training Logistic Regression...\n",
      "Results for Logistic Regression:\n",
      "Accuracy: 0.7177\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.90      0.80       653\n",
      "           1       0.77      0.64      0.70       351\n",
      "           2       0.43      0.17      0.24       165\n",
      "\n",
      "    accuracy                           0.72      1169\n",
      "   macro avg       0.64      0.57      0.58      1169\n",
      "weighted avg       0.69      0.72      0.69      1169\n",
      "\n",
      "Confusion Matrix:\n",
      "[[585  38  30]\n",
      " [118 226   7]\n",
      " [106  31  28]]\n",
      "--------------------------------------------------\n",
      "Training Random Forest...\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.6578\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.81      0.74       653\n",
      "           1       0.82      0.60      0.69       351\n",
      "           2       0.20      0.16      0.18       165\n",
      "\n",
      "    accuracy                           0.66      1169\n",
      "   macro avg       0.57      0.52      0.54      1169\n",
      "weighted avg       0.65      0.66      0.65      1169\n",
      "\n",
      "Confusion Matrix:\n",
      "[[532  24  97]\n",
      " [134 211   6]\n",
      " [116  23  26]]\n",
      "--------------------------------------------------\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [11:22:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for XGBoost:\n",
      "Accuracy: 0.6595\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.86      0.75       653\n",
      "           1       0.80      0.53      0.64       351\n",
      "           2       0.26      0.16      0.20       165\n",
      "\n",
      "    accuracy                           0.66      1169\n",
      "   macro avg       0.58      0.51      0.53      1169\n",
      "weighted avg       0.65      0.66      0.64      1169\n",
      "\n",
      "Confusion Matrix:\n",
      "[[560  25  68]\n",
      " [159 185   7]\n",
      " [119  20  26]]\n",
      "--------------------------------------------------\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11023\n",
      "[LightGBM] [Info] Number of data points in the train set: 4673, number of used features: 513\n",
      "[LightGBM] [Info] Start training from score -0.634753\n",
      "[LightGBM] [Info] Start training from score -1.135670\n",
      "[LightGBM] [Info] Start training from score -1.905645\n",
      "Results for LightGBM:\n",
      "Accuracy: 0.6441\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.81      0.74       653\n",
      "           1       0.72      0.56      0.63       351\n",
      "           2       0.23      0.15      0.18       165\n",
      "\n",
      "    accuracy                           0.64      1169\n",
      "   macro avg       0.54      0.51      0.52      1169\n",
      "weighted avg       0.63      0.64      0.63      1169\n",
      "\n",
      "Confusion Matrix:\n",
      "[[531  49  73]\n",
      " [141 197  13]\n",
      " [112  28  25]]\n",
      "--------------------------------------------------\n",
      "Training SVM...\n",
      "Results for SVM:\n",
      "Accuracy: 0.7228\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.90      0.80       653\n",
      "           1       0.77      0.65      0.70       351\n",
      "           2       0.49      0.17      0.25       165\n",
      "\n",
      "    accuracy                           0.72      1169\n",
      "   macro avg       0.66      0.57      0.59      1169\n",
      "weighted avg       0.70      0.72      0.70      1169\n",
      "\n",
      "Confusion Matrix:\n",
      "[[589  42  22]\n",
      " [116 228   7]\n",
      " [110  27  28]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run & test models\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    print(f\"Training {name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    \n",
    "    model.fit(X_train, y_train) \n",
    "\n",
    "    # Predict on the test sbet\n",
    "    \n",
    "    y_pred = model.predict(X_test)  \n",
    "    \n",
    "    # Evaluate the model\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    \n",
    "    results[name] = {\n",
    "        \n",
    "        \"accuracy\": accuracy,\n",
    "        \n",
    "        \"classification_report\": report,\n",
    "        \n",
    "        \"confusion_matrix\": confusion\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    \n",
    "    print(f\"Results for {name}:\")\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    \n",
    "    print(confusion)\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977f669-cd0f-49a3-81fb-3bd1e9fbb497",
   "metadata": {},
   "source": [
    "_Comment_: Linear models seem to perform better here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c2364-4fb8-4cf4-ae4e-541adbd2bf21",
   "metadata": {},
   "source": [
    "#### Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0133a-12be-4483-9d65-c322398db82c",
   "metadata": {},
   "source": [
    "The project progressed smoothly overall, with no significant issues. However, a minor challenge arose when the text data was stored in a list format after being processed, which led to difficulties during the initialization of the TF-IDF vectors. After some troubleshooting, I identified the format issue and converted the text data from a list to a string format. Once resolved, I was able to proceed successfully to the modeling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd32e6b-1553-416a-91d4-758395832a85",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec3e98-527a-4748-9a5e-2be414e4b743",
   "metadata": {},
   "source": [
    "The top-performing models in this analysis are the Logistic Regression and Support Vector Machine (SVM), both of which exhibit near-identical performance across all evaluation metrics—accuracy, precision, recall, and F1 score. \n",
    "\n",
    "Both models are particularly effective at identifying neutral instances, which is expected due to the high representation of neutral observations in the dataset. However, both models face challenges in accurately classifying positive and negative sentences, with a notable struggle in detecting negative sentiment. This performance is anticipated, given that these models are relatively simple and do not incorporate advanced techniques such as under- or over-sampling to address the class imbalance in the dataset. The performance of these models could likely be improved with the integration of such techniques.\n",
    "\n",
    "Although the SVM model achieved slightly better performance than the multinomial logistic model, I would personally prefer the multinomial logistic model due to its faster computation. Based on the current results, it proves to be more efficient overall. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "panel-cell-order": [
   "6b041909-f882-41d4-8041-839fcc2abf18",
   "e2b5e2c3-78db-4722-8d42-d72afe01ed27",
   "4672f706-0fc9-4c08-9b50-9a9199e96a1e",
   "9217063f-3076-476f-97f0-66917d89f171",
   "73f14148-c27f-4b70-88b8-0a5da07cdfa5",
   "1b02ece6-91e7-45ee-b0ea-9859c9352e26",
   "c0678450-a50c-4af5-ba8a-0e067666153d",
   "22ab6c4b-0773-40cd-8a8e-28c2a4fcc0a6"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
